{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c96d40d3a0f4ca7a27846ba4ec9173e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d32fabaf6eb948be8f497667d6598b0c",
              "IPY_MODEL_7be8956779664136aef71f21a7df59e2",
              "IPY_MODEL_7da84bc48a8047b3b4d30d88af81fb5e"
            ],
            "layout": "IPY_MODEL_b99983fada9141b99a27abb099b7762e"
          }
        },
        "d32fabaf6eb948be8f497667d6598b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95dac0e23d54365ac323014a7be28e7",
            "placeholder": "​",
            "style": "IPY_MODEL_424f4ee96e5a454488d4e71cf14d9c53",
            "value": "Fetching 2 files: 100%"
          }
        },
        "7be8956779664136aef71f21a7df59e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6d8d4607e841afa9f02d0e8f9eb37e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab6729ad519741be864cfec3da2af770",
            "value": 2
          }
        },
        "7da84bc48a8047b3b4d30d88af81fb5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaadef5868804fefa4692fdcd404b51a",
            "placeholder": "​",
            "style": "IPY_MODEL_26b9b90ff1c44b4dbec4dd285e7e946e",
            "value": " 2/2 [00:00&lt;00:00, 207.77it/s]"
          }
        },
        "b99983fada9141b99a27abb099b7762e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a95dac0e23d54365ac323014a7be28e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424f4ee96e5a454488d4e71cf14d9c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6d8d4607e841afa9f02d0e8f9eb37e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6729ad519741be864cfec3da2af770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaadef5868804fefa4692fdcd404b51a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b9b90ff1c44b4dbec4dd285e7e946e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aea9c416cd394c8197587eae7246226c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f2bfbe4bb6b4152ad8d57c51d8700fa",
              "IPY_MODEL_1771e5239dad424a9327ba254e633bf4",
              "IPY_MODEL_afba3190b62b4ba4a4753773342d755b"
            ],
            "layout": "IPY_MODEL_eae613a57bd947469d0bfba858885fe7"
          }
        },
        "6f2bfbe4bb6b4152ad8d57c51d8700fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0dbfcdcabc4418b00c7c5edf82af1a",
            "placeholder": "​",
            "style": "IPY_MODEL_640b04d0707a4444890bffe400ee84a4",
            "value": "Fetching 2 files: 100%"
          }
        },
        "1771e5239dad424a9327ba254e633bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b1bea2deca40c6a37bdeae333dab68",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4749ac2e00da4867a6a3effa2a748272",
            "value": 2
          }
        },
        "afba3190b62b4ba4a4753773342d755b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9915c45d57a94671b50e3d5bb52fcabc",
            "placeholder": "​",
            "style": "IPY_MODEL_5ffb9b29ab22491c8d5f6eea95a1cf7e",
            "value": " 2/2 [00:00&lt;00:00, 189.54it/s]"
          }
        },
        "eae613a57bd947469d0bfba858885fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e0dbfcdcabc4418b00c7c5edf82af1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640b04d0707a4444890bffe400ee84a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00b1bea2deca40c6a37bdeae333dab68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4749ac2e00da4867a6a3effa2a748272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9915c45d57a94671b50e3d5bb52fcabc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ffb9b29ab22491c8d5f6eea95a1cf7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime-gpu num2words"
      ],
      "metadata": {
        "id": "EUlDqlOAjgdK",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a0e24a-848a-49b8-e3aa-b5921cc2eaf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.0)\n",
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.12/dist-packages (1.22.0)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.12/dist-packages (0.5.14)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.13.3)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from num2words) (0.6.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/huggingface/lerobot.git\n",
        "! cd lerobot && pip install ."
      ],
      "metadata": {
        "id": "lrioV7iak3Fm",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb97e79-5e3d-44a2-dd84-c128a51c097a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lerobot' already exists and is not an empty directory.\n",
            "Processing /content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (4.0.0)\n",
            "Requirement already satisfied: diffusers>=0.27.2 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.35.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.2 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (0.35.0)\n",
            "Requirement already satisfied: cmake>=3.29.0.1 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (3.31.6)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.8.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (4.12.0.88)\n",
            "Requirement already satisfied: av>=14.2.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (15.1.0)\n",
            "Requirement already satisfied: jsonlines>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (4.0.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (25.0)\n",
            "Requirement already satisfied: pynput>=1.7.7 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (1.8.1)\n",
            "Requirement already satisfied: pyserial>=3.5 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (3.5)\n",
            "Requirement already satisfied: wandb>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.21.4)\n",
            "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (2.7.1)\n",
            "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.5)\n",
            "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.22.1)\n",
            "Requirement already satisfied: draccus==0.10.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.10.0)\n",
            "Requirement already satisfied: gymnasium<1.0.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.29.1)\n",
            "Requirement already satisfied: rerun-sdk<0.23.0,>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (0.22.1)\n",
            "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (8.6.1)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.3.4) (2.37.0)\n",
            "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from lerobot==0.3.4) (3.1.0)\n",
            "Requirement already satisfied: mergedeep~=1.3 in /usr/local/lib/python3.12/dist-packages (from draccus==0.10.0->lerobot==0.3.4) (1.3.4)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.12/dist-packages (from draccus==0.10.0->lerobot==0.3.4) (6.0.2)\n",
            "Requirement already satisfied: pyyaml-include~=1.4 in /usr/local/lib/python3.12/dist-packages (from draccus==0.10.0->lerobot==0.3.4) (1.4.1)\n",
            "Requirement already satisfied: toml~=0.10 in /usr/local/lib/python3.12/dist-packages (from draccus==0.10.0->lerobot==0.3.4) (0.10.2)\n",
            "Requirement already satisfied: typing-inspect~=0.9.0 in /usr/local/lib/python3.12/dist-packages (from draccus==0.10.0->lerobot==0.3.4) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->lerobot==0.3.4) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (2025.3.0)\n",
            "Requirement already satisfied: orderly-set<6,>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.3.4) (5.5.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.27.2->lerobot==0.3.4) (8.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.27.2->lerobot==0.3.4) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.27.2->lerobot==0.3.4) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.27.2->lerobot==0.3.4) (11.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4) (0.0.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.2->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (1.1.10)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (0.3.4)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (0.1.9)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (3.0.52)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.3.4) (0.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.3.4) (5.9.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=4.0.0->lerobot==0.3.4) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pynput>=1.7.7->lerobot==0.3.4) (1.17.0)\n",
            "Requirement already satisfied: evdev>=1.3 in /usr/local/lib/python3.12/dist-packages (from pynput>=1.7.7->lerobot==0.3.4) (1.9.2)\n",
            "Requirement already satisfied: python-xlib>=0.17 in /usr/local/lib/python3.12/dist-packages (from pynput>=1.7.7->lerobot==0.3.4) (0.33)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.8.0,>=2.2.1->lerobot==0.3.4) (3.3.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (2.11.9)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.20.0->lerobot==0.3.4) (2.38.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.20.0->lerobot==0.3.4) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.3.4) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.3.4) (1.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers>=0.27.2->lerobot==0.3.4) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.3.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->lerobot==0.3.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->lerobot==0.3.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->lerobot==0.3.4) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.20.0->lerobot==0.3.4) (5.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4) (0.2.13)\n",
            "Building wheels for collected packages: lerobot\n",
            "  Building wheel for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.3.4-py3-none-any.whl size=674726 sha256=a0ca57a81ad1789151bbafee2ae40d3b87a9991d29a85466380df69bae8ce6a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h3srnh2r/wheels/09/b4/fe/75732b1d640db96ba1f856f2b7328b232a03b696a39cb59686\n",
            "Successfully built lerobot\n",
            "Installing collected packages: lerobot\n",
            "  Attempting uninstall: lerobot\n",
            "    Found existing installation: lerobot 0.3.4\n",
            "    Uninstalling lerobot-0.3.4:\n",
            "      Successfully uninstalled lerobot-0.3.4\n",
            "Successfully installed lerobot-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q ipdb\n",
        "%env PYTHONBREAKPOINT=IPython.core.debugger.set_trace  # makes breakpoint() use ipdb\n",
        "%pdb on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpd-rcyoQCXE",
        "outputId": "f23157e8-0574-4408-fcab-739b7f3f5ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONBREAKPOINT=IPython.core.debugger.set_trace  # makes breakpoint() use ipdb\n",
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
        "from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig\n",
        "\n",
        "config = SmolVLAConfig\n",
        "policy = SmolVLAPolicy.from_pretrained(\"lerobot/smolvla_base\")"
      ],
      "metadata": {
        "id": "We9609DxIgHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "5c96d40d3a0f4ca7a27846ba4ec9173e",
            "d32fabaf6eb948be8f497667d6598b0c",
            "7be8956779664136aef71f21a7df59e2",
            "7da84bc48a8047b3b4d30d88af81fb5e",
            "b99983fada9141b99a27abb099b7762e",
            "a95dac0e23d54365ac323014a7be28e7",
            "424f4ee96e5a454488d4e71cf14d9c53",
            "1d6d8d4607e841afa9f02d0e8f9eb37e",
            "ab6729ad519741be864cfec3da2af770",
            "eaadef5868804fefa4692fdcd404b51a",
            "26b9b90ff1c44b4dbec4dd285e7e946e",
            "aea9c416cd394c8197587eae7246226c",
            "6f2bfbe4bb6b4152ad8d57c51d8700fa",
            "1771e5239dad424a9327ba254e633bf4",
            "afba3190b62b4ba4a4753773342d755b",
            "eae613a57bd947469d0bfba858885fe7",
            "8e0dbfcdcabc4418b00c7c5edf82af1a",
            "640b04d0707a4444890bffe400ee84a4",
            "00b1bea2deca40c6a37bdeae333dab68",
            "4749ac2e00da4867a6a3effa2a748272",
            "9915c45d57a94671b50e3d5bb52fcabc",
            "5ffb9b29ab22491c8d5f6eea95a1cf7e"
          ]
        },
        "outputId": "1074bc4c-3c08-4477-aa0f-3e905b9cc04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n",
            "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c96d40d3a0f4ca7a27846ba4ec9173e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aea9c416cd394c8197587eae7246226c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reducing the number of VLM layers to 16 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Missing key(s) when loading model: {'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.norm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.action_time_mlp_out.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.action_out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.norm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight', 'model.action_time_mlp_in.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight', 'model.action_in_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight', 'model.state_proj.bias', 'model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.embed_tokens.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight', 'model.action_out_proj.bias', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight', 'model.state_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight', 'model.action_time_mlp_in.bias', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight', 'model.action_time_mlp_out.bias', 'model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias', 'model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight', 'model.action_in_proj.bias', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vlm_with_expert.vlm.lm_head.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias'}\n",
            "WARNING:root:Unexpected key(s) when loading model: {'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight', 'unnormalize_outputs.so100_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight', 'normalize_inputs.so100_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'unnormalize_outputs.so100-blue_buffer_action.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model._orig_mod.action_time_mlp_out.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight', 'model._orig_mod.action_in_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.lm_head.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight', 'model._orig_mod.action_time_mlp_out.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'model._orig_mod.action_out_proj.weight', 'normalize_targets.so100_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight', 'normalize_targets.so100_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight', 'unnormalize_outputs.so100-red_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.norm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight', 'model._orig_mod.state_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias', 'model._orig_mod.action_out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'normalize_targets.so100-blue_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight', 'normalize_targets.so100-red_buffer_action.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'unnormalize_outputs.so100_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight', 'normalize_targets.so100-red_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight', 'normalize_inputs.so100_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight', 'normalize_inputs.so100-red_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight', 'model._orig_mod.state_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'model._orig_mod.action_time_mlp_in.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.embed_tokens.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight', 'unnormalize_outputs.so100-blue_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.norm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'model._orig_mod.action_in_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'unnormalize_outputs.so100-red_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias', 'normalize_targets.so100-blue_buffer_action.std', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight', 'normalize_inputs.so100-red_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight', 'normalize_inputs.so100-blue_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight', 'model._orig_mod.action_time_mlp_in.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight', 'normalize_inputs.so100-blue_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from lerobot.policies.smolvla.smolvlm_with_expert import SmolVLMWithExpertModel\n",
        "\n",
        "from typing import Optional, List"
      ],
      "metadata": {
        "id": "XyUrpVZ3FRZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StateProjector(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.module = policy.model.state_proj\n",
        "\n",
        "  def forward(self, state):\n",
        "    return self.module(state)"
      ],
      "metadata": {
        "id": "RcUDGLENI8ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionProjector(nn.Module):\n",
        "  def __init__(self, mode=\"IN\"):\n",
        "    super().__init__()\n",
        "    if mode == \"IN\":\n",
        "      self.module = policy.model.action_in_proj\n",
        "    elif mode == \"OUT\":\n",
        "      self.module = policy.model.action_out_proj\n",
        "    elif mode == \"TIME_IN\":\n",
        "      self.module = policy.model.action_time_mlp_in\n",
        "    elif mode == \"TIME_OUT\":\n",
        "      self.module = policy.model.action_time_mlp_out\n",
        "\n",
        "  def forward(self, state):\n",
        "    return self.module(state)"
      ],
      "metadata": {
        "id": "wtJsLK6RJv6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmolVLMVision(nn.Module):\n",
        "    def __init__(self, model_id, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = policy.model.vlm_with_expert\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        return self.model.embed_image(image)"
      ],
      "metadata": {
        "id": "IccXNoYAFPS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmolVLMText(nn.Module):\n",
        "    def __init__(self, model_id, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = policy.model.vlm_with_expert\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        return self.model.embed_language_tokens(tokens)"
      ],
      "metadata": {
        "id": "x9OVRPWMFhXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmolVLMForONNXExport(nn.Module):\n",
        "    def __init__(self, model_id, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = policy.model.vlm_with_expert\n",
        "\n",
        "        self.config = SmolVLAConfig\n",
        "\n",
        "        # Bake in the configuration for the static graph\n",
        "        self.attention_mode = self.config.vlm_model_name\n",
        "        self.num_vlm_layers = self.model.num_vlm_layers\n",
        "        self.num_expert_layers = self.model.num_expert_layers\n",
        "        self.fill_kv_cache = False\n",
        "        self.use_cache = True\n",
        "\n",
        "    def embed_img(self,\n",
        "                  image: torch.Tensor):\n",
        "          return self.model.embed_image(image)\n",
        "\n",
        "    def embed_text(self,\n",
        "                   tokens: torch.Tensor):\n",
        "        return self.model.embed_language_tokens(tokens)\n",
        "\n",
        "    def forward(\n",
        "          self,\n",
        "          vlm_embeds: torch.Tensor,\n",
        "          expert_embeds: torch.Tensor,\n",
        "          attention_mask: torch.Tensor,\n",
        "          position_ids: torch.LongTensor,\n",
        "          # Flattened past_key_values: one tensor for keys, one for values per layer\n",
        "          *past_key_values_flat: torch.Tensor):\n",
        "\n",
        "\n",
        "          inputs_embeds = [vlm_embeds, expert_embeds]\n",
        "\n",
        "          past_key_values = {}\n",
        "          if past_key_values_flat:\n",
        "              # Logic to reconstruct the dictionary from the flat tuple of tensors\n",
        "              # This assumes a consistent ordering: (key_layer0, val_layer0, key_layer1, val_layer1, ...)\n",
        "              for i in range(0, len(past_key_values_flat), 2):\n",
        "                  layer_idx = i // 2\n",
        "                  past_key_values[layer_idx] = {\n",
        "                      \"key_states\": past_key_values_flat[i],\n",
        "                      \"value_states\": past_key_values_flat[i+1],\n",
        "                  }\n",
        "          else:\n",
        "              past_key_values = None\n",
        "\n",
        "\n",
        "          final_embeds_list, new_past_key_values = self.model.forward(\n",
        "              attention_mask=attention_mask,\n",
        "              position_ids=position_ids,\n",
        "              past_key_values=past_key_values,\n",
        "              inputs_embeds=inputs_embeds,\n",
        "              use_cache=self.use_cache,\n",
        "              fill_kv_cache=self.fill_kv_cache,\n",
        "          )\n",
        "\n",
        "          # --- 3. Flatten Outputs ---\n",
        "          # The output must be a flat tuple of tensors\n",
        "          vlm_output_embeds = final_embeds_list[0]\n",
        "          expert_output_embeds = final_embeds_list[1] # expects to be None\n",
        "\n",
        "          present_key_values_flat = []\n",
        "          if new_past_key_values:\n",
        "            for i in range(self.num_vlm_layers):\n",
        "                if i in new_past_key_values:\n",
        "                    present_key_values_flat.append(new_past_key_values[i][\"key_states\"])\n",
        "                    present_key_values_flat.append(new_past_key_values[i][\"value_states\"])\n",
        "\n",
        "          return (vlm_output_embeds, expert_output_embeds, *present_key_values_flat)"
      ],
      "metadata": {
        "id": "SoQVIbHEFu8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n"
      ],
      "metadata": {
        "id": "_l7b3AANZwXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PrefilL model condifuration"
      ],
      "metadata": {
        "id": "4cXEjUqABgex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = SmolVLMForONNXExport(\"\").to(torch.float32)\n",
        "onnx_model.eval()\n",
        "\n",
        "onnx_model.fill_kv_cache = True\n",
        "\n",
        "onnx_model.to(\"cuda:0\")\n",
        "\n",
        "# output_names = [\"vlm_output_embeds\", \"expert_output_embeds\"]\n",
        "output_names = [\"vlm_output_embeds\"]\n",
        "num_layers = onnx_model.model.num_vlm_layers\n",
        "for i in range(num_layers):\n",
        "    output_names.append(f\"present_key_{i}\")\n",
        "    output_names.append(f\"present_value_{i}\")\n",
        "\n",
        "vlm_hidden_dim = onnx_model.model.config.text_config.hidden_size\n",
        "expert_hidden_dim = onnx_model.model.expert_hidden_size\n",
        "\n",
        "batch_size = 1\n",
        "vlm_seq_len = 256\n",
        "expert_seq_len = 16\n",
        "total_seq_len = vlm_seq_len + expert_seq_len\n",
        "\n",
        "dummy_vlm_embeds = torch.randn(batch_size, vlm_seq_len, 960, device=\"cuda:0\", dtype=torch.float32)\n",
        "# dummy_expert_embeds = torch.randn(batch_size, expert_seq_len, 320, device=\"cuda:0\", dtype=torch.bfloat16) # input_embeds\n",
        "dummy_expert_embeds = torch.randn(batch_size, expert_seq_len, expert_hidden_dim, device=\"cuda:0\", dtype=torch.float32) # input_embeds\n",
        "# dummy_expert_embeds = torch.randn(batch_size, 320, expert_hidden_dim, device=\"cuda:0\", dtype=torch.bfloat16) # input_embeds\n",
        "dummy_attn_mask = torch.ones(batch_size, total_seq_len, total_seq_len, device=\"cuda:0\", dtype=torch.bool)\n",
        "dummy_pos_ids = torch.arange(total_seq_len, device=\"cuda:0\").unsqueeze(0)\n",
        "\n",
        "\n",
        "# dummy_inputs = (dummy_vlm_embeds, dummy_expert_embeds, dummy_attn_mask, dummy_pos_ids)\n",
        "dummy_inputs = (dummy_vlm_embeds, None, dummy_attn_mask, dummy_pos_ids)\n",
        "\n",
        "# we ant past_key_values from it, the second output\n",
        "\n"
      ],
      "metadata": {
        "id": "CCuA_D3JwuzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(\n",
        "    onnx_model,\n",
        "    dummy_inputs,\n",
        "    \"smolvlm_expert_prefill.onnx\",\n",
        "    input_names=[\"vlm_embeds\", \"expert_embeds\", \"attention_mask\", \"position_ids\"],\n",
        "    output_names=output_names,\n",
        "    opset_version=17\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTp-UHOZPQnU",
        "outputId": "1478bfd7-848f-481a-b3b5-5083b108776f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lerobot/policies/smolvla/smolvlm_with_expert.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_len < position_ids.shape[1]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx.checker.check_model(\"smolvlm_expert_prefill.onnx\")"
      ],
      "metadata": {
        "id": "pFo-lfHCbQok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, *past_key_values = onnx_model.forward(*dummy_inputs)"
      ],
      "metadata": {
        "id": "P_BPscyKOrFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "past_key_values[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wKrzPx8x6-J",
        "outputId": "32bee17d-e8bd-44b2-cf15-69d34c5335c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 5, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "onnx_model = SmolVLMForONNXExport(\"\")"
      ],
      "metadata": {
        "id": "SafSe3BbfIgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode model configuration and export"
      ],
      "metadata": {
        "id": "U45TbSvfBnAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model.to(torch.float32).to(\"cuda:0\")\n",
        "onnx_model.eval()\n",
        "onnx_model.fill_kv_cache = False\n",
        "onnx_model.use_cache = True\n",
        "current_seq_len = 1\n",
        "past_seq_len = 256\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "vlm_hidden_dim = onnx_model.model.config.text_config.hidden_size\n",
        "expert_hidden_dim = onnx_model.model.expert_hidden_size\n",
        "num_layers = onnx_model.model.num_vlm_layers\n",
        "num_kv_heads = onnx_model.model.config.text_config.num_key_value_heads\n",
        "head_dim = vlm_hidden_dim // onnx_model.model.config.text_config.num_attention_heads\n",
        "\n",
        "dummy_vlm_embeds = torch.randn(batch_size, current_seq_len, 960, device=\"cuda:0\", dtype=torch.float32)\n",
        "dummy_expert_embeds = torch.randn(batch_size, current_seq_len, expert_hidden_dim, device=\"cuda:0\", dtype=torch.float32)\n",
        "# dummy_attn_mask = torch.ones(batch_size, current_seq_len, past_seq_len + current_seq_len, device=\"cuda:0\", dtype=torch.bool)\n",
        "dummy_attn_mask = torch.ones(batch_size, current_seq_len, current_seq_len, device=\"cuda:0\", dtype=torch.bool)\n",
        "# dummy_attn_mask = torch.ones(batch_size, 1, past_seq_len, past_seq_len + current_seq_len + 1, device=\"cuda:0\", dtype=torch.bool)\n",
        "# dummy_attn_mask = torch.ones(batch_size, 1, past_seq_len + current_seq_len, device=\"cuda:0\", dtype=torch.bool)\n",
        "# dummy_pos_ids = torch.arange(past_seq_len + current_seq_len, device=\"cuda:0\").unsqueeze(0)\n",
        "dummy_pos_ids = torch.arange(current_seq_len, device=\"cuda:0\").unsqueeze(0)\n",
        "\n",
        "print(past_seq_len + current_seq_len)\n",
        "print(total_seq_len)\n",
        "\n",
        "input_names = [\"vlm_embeds\", \"expert_embeds\", \"attention_mask\", \"position_ids\"]\n",
        "\n",
        "# past_key_values_flat = []\n",
        "# for _ in range(num_layers):\n",
        "#     key_shape = (batch_size, num_kv_heads, past_seq_len, head_dim)\n",
        "#     val_shape = (batch_size, num_kv_heads, past_seq_len, head_dim)\n",
        "#     past_key_values_flat.append(torch.randn(key_shape, device=\"cuda:0\", dtype=torch.bfloat16))\n",
        "#     past_key_values_flat.append(torch.randn(val_shape, device=\"cuda:0\", dtype=torch.bfloat16))\n",
        "\n",
        "dummy_inputs = (\n",
        "    # dummy_vlm_embeds,\n",
        "    None,\n",
        "    dummy_expert_embeds,\n",
        "    dummy_attn_mask,\n",
        "    dummy_pos_ids,\n",
        "    *past_key_values\n",
        "    # None\n",
        ")\n",
        "\n",
        "\n",
        "for i in range(num_layers):\n",
        "    input_names.append(f\"past_key_{i}\")\n",
        "    input_names.append(f\"past_value_{i}\")\n",
        "\n",
        "\n",
        "# Outputs include the embeddings and the *updated* KV cache\n",
        "# output_names = [\"vlm_output_embeds\", \"expert_output_embeds\"]\n",
        "output_names = [\"expert_output_embeds\"]\n",
        "for i in range(num_layers):\n",
        "    # The output names must be consistent for the next iteration\n",
        "    output_names.append(f\"present_key_{i}\")\n",
        "    output_names.append(f\"present_value_{i}\")\n",
        "\n",
        "# dynamic_axes = {\"vlm_embeds\": {}, \"expert_embeds\": {}} # Add other static inputs if needed\n",
        "# for i in range(num_layers):\n",
        "#     # The '2' axis is the sequence length dimension in the KV cache shape\n",
        "#     # (batch_size, num_heads, sequence_length, head_dim)\n",
        "#     dynamic_axes[f\"past_key_{i}\"] = {2: \"past_sequence_length\"}\n",
        "#     dynamic_axes[f\"past_value_{i}\"] = {2: \"past_sequence_length\"}\n",
        "#     dynamic_axes[f\"present_key_{i}\"] = {2: \"total_sequence_length\"}\n",
        "#     dynamic_axes[f\"present_value_{i}\"] = {2: \"total_sequence_length\"}\n",
        "\n",
        "# we need output_embeds - first output\n",
        "torch.onnx.export(\n",
        "    onnx_model,\n",
        "    dummy_inputs,\n",
        "    \"smolvlm_expert_decode.onnx\",\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    # dynamic_axes=dynamic_axes,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "_I5eZPVepZbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9875eff4-a1f5-455d-f404-e47ddd8e6299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257\n",
            "272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model.model.get_vlm_model().vision_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWz23cb9Ed1t",
        "outputId": "0111e1f8-fa8d-4288-e83a-442b57f7c5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SmolVLMVisionTransformer(\n",
              "  (embeddings): SmolVLMVisionEmbeddings(\n",
              "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
              "    (position_embedding): Embedding(1024, 768)\n",
              "  )\n",
              "  (encoder): SmolVLMEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x SmolVLMEncoderLayer(\n",
              "        (self_attn): SmolVLMVisionAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): SmolVLMVisionMLP(\n",
              "          (activation_fn): PytorchGELUTanh()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_image = torch.ones(1, 3, 512, 512, device=\"cuda:0\", dtype=torch.uint8) * 255\n",
        "img_model = SmolVLMVision(\"\")\n",
        "img_model.to(\"cuda:0\")\n",
        "img_model.eval()\n",
        "\n",
        "emb = img_model.forward(dummy_image)\n",
        "print(emb.shape)\n",
        "\n",
        "torch.onnx.export(\n",
        "    img_model,\n",
        "    dummy_image,\n",
        "    \"smolvlm_vision.onnx\",\n",
        "    input_names=[\"image\"],\n",
        "    output_names=[\"output_embeds\"],\n",
        "    opset_version=17,\n",
        "    do_constant_folding=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObBVGCfpCMJs",
        "outputId": "07071b25-d8d6-443d-fc4a-1024d4dc022e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 960])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py:145: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for batch_idx, p_attn_mask in enumerate(patch_attention_mask):\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py:532: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  height = width = int(seq**0.5)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py:538: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  x = x.reshape(bsz, int(seq / (scale_factor**2)), embed_dim * (scale_factor**2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import helper, TensorProto, shape_inference\n",
        "\n",
        "MODEL_IN = \"smolvlm_vision.onnx\"\n",
        "MODEL_OUT = \"smolvlm_vision.onnx\"\n",
        "\n",
        "# Names from your error:\n",
        "BAD_VALUE = \"/vision_model/embeddings/ScatterND_output_0\"\n",
        "GATHER_NODE_NAME = \"/vision_model/embeddings/position_embedding/Gather\"\n",
        "\n",
        "m = onnx.load(MODEL_IN)\n",
        "\n",
        "# 1) Create a Cast node that converts BAD_VALUE -> INT64\n",
        "cast_out = BAD_VALUE + \"_int64\"\n",
        "cast_node = helper.make_node(\n",
        "    \"Cast\",\n",
        "    inputs=[BAD_VALUE],\n",
        "    outputs=[cast_out],\n",
        "    name=BAD_VALUE + \"_CastToInt64\",\n",
        "    to=TensorProto.INT64,\n",
        ")\n",
        "\n",
        "# 2) Insert the Cast and rewire the target Gather's indices input\n",
        "#    (Gather inputs: [data, indices, axis?] -> indices is input[1])\n",
        "for i, n in enumerate(m.graph.node):\n",
        "    if n.name == GATHER_NODE_NAME:\n",
        "        # Insert cast before rewire to preserve topological order\n",
        "        m.graph.node.insert(i, cast_node)\n",
        "        n.input[1] = cast_out\n",
        "        break\n",
        "else:\n",
        "    raise RuntimeError(f\"Couldn't find node named {GATHER_NODE_NAME!r}\")\n",
        "\n",
        "# 3) (Optional but recommended) run shape inference & validity checks\n",
        "m = shape_inference.infer_shapes(m)\n",
        "onnx.checker.check_model(m)\n",
        "\n",
        "onnx.save(m, MODEL_OUT)\n",
        "print(f\"Saved fixed model to {MODEL_OUT}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jobzCpSU-cL",
        "outputId": "09e19cee-07b6-40dc-8093-a941384374a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fixed model to smolvlm_vision.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# providers = [\"CUDAExecutionProvider\"] if torch.cuda.is_available() else [\"CPUExecutionProvider\"]\n",
        "providers = [\"CPUExecutionProvider\"]\n",
        "\n",
        "vision_session = ort.InferenceSession(\"smolvlm_vision.onnx\", providers=providers)\n",
        "\n",
        "image = np.ones((1, 3, 512, 512), dtype=np.uint8)\n",
        "\n",
        "input_name = vision_session.get_inputs()[0].name\n",
        "\n",
        "onnx_outputs = vision_session.run(None, {input_name: image})\n",
        "\n",
        "# The output is a list, get the first element which should be the embeddings\n",
        "# onnx_embs = onnx_outputs[0]\n",
        "\n",
        "# assert((embs.cpu() == onnx_embs).all()"
      ],
      "metadata": {
        "id": "KKYYFYThIrlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = SmolVLMText(\"\").to(torch.float32).cuda()\n",
        "processor = text_model.model.processor\n",
        "tokens = processor(text=\"Hello this is a test text prompt\", return_tensors=\"pt\")\n",
        "print(f\"Text tokens = {tokens}\")\n",
        "\n",
        "embs = text_model.forward(tokens[\"input_ids\"].cuda())\n",
        "print(f\"Shpae of text embeddings = {emb.shape}\")\n",
        "\n",
        "torch.onnx.export(\n",
        "    text_model,\n",
        "    tokens[\"input_ids\"].cuda(),\n",
        "    \"smolvlm_text.onnx\",\n",
        "    input_names=[\"tokens\"],\n",
        "    output_names=[\"output_embeds\"],\n",
        "    dynamic_axes={\"tokens\": {0: \"batch\", 1: \"sequence_length\"}, \"output_embeds\": {0: \"batch\", 1: \"sequence_length\"}},\n",
        "    opset_version=17,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWgMb8hq2eUU",
        "outputId": "1360e70f-4c3f-4023-b01d-23453015d4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text tokens = {'input_ids': tensor([[19556,   451,   314,   253,  1028,  1694,  6011]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "Shpae of text embeddings = torch.Size([1, 64, 960])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if onnx outputs and torch outputs are equal"
      ],
      "metadata": {
        "id": "RrQkwufYbIze"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3859c109"
      },
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# providers = [\"CUDAExecutionProvider\"] if torch.cuda.is_available() else [\"CPUExecutionProvider\"]\n",
        "providers = [\"CPUExecutionProvider\"]\n",
        "\n",
        "text_session = ort.InferenceSession(\"smolvlm_text.onnx\", providers=providers)\n",
        "\n",
        "tokens = processor(text=\"Hello this is a test text prompt\", return_tensors=\"np\")\n",
        "\n",
        "input_name = text_session.get_inputs()[0].name\n",
        "\n",
        "onnx_outputs = text_session.run(None, {input_name: tokens[\"input_ids\"]})\n",
        "\n",
        "# The output is a list, get the first element which should be the embeddings\n",
        "onnx_embs = onnx_outputs[0]\n",
        "\n",
        "assert((embs.cpu() == onnx_embs).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policy.config.n_obs_steps\n",
        "policy.config.input_features['observation.state'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW5l9htqbbJ0",
        "outputId": "a6a5af20-f868-4a13-c54a-947edf6d46d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_projector = StateProjector().to(torch.float32).to(\"cuda:0\")\n",
        "input = torch.randn(1, policy.config.input_features['observation.state'].shape[0], 32, device=\"cuda:0\") # shape seems correct, but need to check it further\n",
        "\n",
        "state_projector.eval()\n",
        "\n",
        "print(state_projector.forward(input).shape)\n",
        "\n",
        "state_projector.forward(input)\n",
        "\n",
        "torch.onnx.export(\n",
        "    state_projector,\n",
        "    input,\n",
        "    \"state_projector.onnx\",\n",
        "    input_names=[\"state\"],\n",
        "    opset_version=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QedSrEQe0WMT",
        "outputId": "936f4dde-7de0-442e-9bb1-9dc7b81b011a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 960])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_in_projector = ActionProjector(mode=\"IN\").to(torch.float32).to(\"cuda:0\")\n",
        "input = torch.randn(1, policy.config.input_features['observation.state'].shape[0], 32, device=\"cuda:0\") # shape seems correct, but need to check it further\n",
        "\n",
        "action_in_projector.eval()\n",
        "\n",
        "action_in_projector.forward(input)\n",
        "\n",
        "torch.onnx.export(\n",
        "    action_in_projector,\n",
        "    input,\n",
        "    \"action_in_projector.onnx\",\n",
        "    input_names=[\"state\"],\n",
        "    opset_version=17)"
      ],
      "metadata": {
        "id": "ytZUzxRWBMHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_out_projector = ActionProjector(mode=\"OUT\").to(torch.float32).to(\"cuda:0\")\n",
        "# input = torch.randn(1, policy.config.input_features['observation.state'].shape[0], policy.config.n_obs_steps)\n",
        "input = torch.randn(1, policy.config.input_features['observation.state'].shape[0], 720, device=\"cuda:0\") # TODO check for the actual shape\n",
        "\n",
        "action_out_projector.eval()\n",
        "\n",
        "print(action_out_projector.forward(input).shape)\n",
        "\n",
        "torch.onnx.export(\n",
        "    action_out_projector,\n",
        "    input,\n",
        "    \"action_out_projector.onnx\",\n",
        "    input_names=[\"action\"],\n",
        "    opset_version=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaV8vIsVlM70",
        "outputId": "ea68f2fd-7166-48f4-c0ba-83a13bc76d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_out_projector = ActionProjector(mode=\"TIME_OUT\").to(torch.float32).to(\"cuda:0\")\n",
        "input = torch.randn(1, 1, 720, device=\"cuda:0\") # TODO check for the actual shape\n",
        "\n",
        "time_out_projector.eval()\n",
        "\n",
        "print(time_out_projector.forward(input).shape)\n",
        "\n",
        "torch.onnx.export(\n",
        "    time_out_projector,\n",
        "    input,\n",
        "    \"time_out_projector.onnx\",\n",
        "    input_names=[\"time\"],\n",
        "    opset_version=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzEpcfdCpF3x",
        "outputId": "0a6dc334-5076-42b0-d19f-fa93466df656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 720])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_in_projector = ActionProjector(mode=\"TIME_IN\").to(torch.float32).to(\"cuda:0\")\n",
        "input = torch.randn(1, 1, 1440, device=\"cuda:0\") # TODO check for the actual shape\n",
        "\n",
        "time_in_projector.eval()\n",
        "\n",
        "print(time_in_projector.forward(input).shape)\n",
        "\n",
        "torch.onnx.export(\n",
        "    time_in_projector,\n",
        "    input,\n",
        "    \"time_in_projector.onnx\",\n",
        "    input_names=[\"time\"],\n",
        "    opset_version=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDVHQjM_rpA9",
        "outputId": "9daed43c-1c36-4d2f-b7a2-e1c8764d249d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 720])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, onnx\n",
        "\n",
        "out_name = None\n",
        "src_name = \"smolvlm_expert_prefill.onnx\"\n",
        "out_dir = \"splited\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "base = out_name or (os.path.splitext(os.path.basename(src_name))[0] + \"_ext.onnx\")\n",
        "out_path = os.path.join(out_dir, base)\n",
        "\n",
        "model = onnx.load(src_name)\n",
        "onnx.save_model(\n",
        "        model,\n",
        "        out_path,\n",
        "        save_as_external_data=True,\n",
        "        all_tensors_to_one_file=True,\n",
        "        location=\"weights.bin\",\n",
        "        size_threshold=0,\n",
        "        convert_attribute=True\n",
        "    )"
      ],
      "metadata": {
        "id": "d9KaKiwD115r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, onnx\n",
        "\n",
        "out_name = None\n",
        "src_name = \"smolvlm_expert_decode.onnx\"\n",
        "out_dir = \"splited_decoder\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "base = out_name or (os.path.splitext(os.path.basename(src_name))[0] + \"_ext.onnx\")\n",
        "out_path = os.path.join(out_dir, base)\n",
        "\n",
        "model = onnx.load(src_name)\n",
        "onnx.save_model(\n",
        "        model,\n",
        "        out_path,\n",
        "        save_as_external_data=True,\n",
        "        all_tensors_to_one_file=True,\n",
        "        location=\"weights.bin\",\n",
        "        size_threshold=0,\n",
        "        convert_attribute=True\n",
        "    )"
      ],
      "metadata": {
        "id": "Y9I6Y1WdFR46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, onnx\n",
        "\n",
        "out_name = None\n",
        "src_name = \"smolvlm_vision.onnx\"\n",
        "out_dir = \"splited_vision\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "base = out_name or (os.path.splitext(os.path.basename(src_name))[0] + \"_ext.onnx\")\n",
        "out_path = os.path.join(out_dir, base)\n",
        "\n",
        "model = onnx.load(src_name)\n",
        "onnx.save_model(\n",
        "        model,\n",
        "        out_path,\n",
        "        save_as_external_data=True,\n",
        "        all_tensors_to_one_file=True,\n",
        "        location=\"weights.bin\",\n",
        "        size_threshold=0,\n",
        "        convert_attribute=True\n",
        ")"
      ],
      "metadata": {
        "id": "n_H5B62cMh4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}